{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rp4x-E5NYoaL"
   },
   "source": [
    "# Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jgx-_LMwvkBe",
    "outputId": "b2abfea9-3eae-48a7-b710-6087770ef469"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (0.14.0)\n",
      "Requirement already satisfied: dash in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (2.18.1)\n",
      "Requirement already satisfied: dash_bootstrap_components in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: pyspark in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: numpy>=1.18 in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from statsmodels) (1.26.4)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.4 in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from statsmodels) (1.10.1)\n",
      "Requirement already satisfied: pandas>=1.0 in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from statsmodels) (2.0.2)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from statsmodels) (0.5.3)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from statsmodels) (23.2)\n",
      "Requirement already satisfied: Flask<3.1,>=1.0.4 in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from dash) (2.2.5)\n",
      "Requirement already satisfied: Werkzeug<3.1 in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from dash) (2.2.3)\n",
      "Requirement already satisfied: plotly>=5.0.0 in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from dash) (5.9.0)\n",
      "Requirement already satisfied: dash-html-components==2.0.0 in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from dash) (2.0.0)\n",
      "Requirement already satisfied: dash-core-components==2.0.0 in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from dash) (2.0.0)\n",
      "Requirement already satisfied: dash-table==5.0.0 in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from dash) (5.0.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from dash) (6.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from dash) (4.12.2)\n",
      "Requirement already satisfied: requests in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from dash) (2.32.2)\n",
      "Requirement already satisfied: retrying in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from dash) (1.3.4)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from dash) (1.6.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from dash) (68.2.2)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: Jinja2>=3.0 in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from Flask<3.1,>=1.0.4->dash) (3.1.2)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from Flask<3.1,>=1.0.4->dash) (2.0.1)\n",
      "Requirement already satisfied: click>=8.0 in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from Flask<3.1,>=1.0.4->dash) (8.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from pandas>=1.0->statsmodels) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from pandas>=1.0->statsmodels) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from pandas>=1.0->statsmodels) (2023.3)\n",
      "Requirement already satisfied: six in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from patsy>=0.5.2->statsmodels) (1.16.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from plotly>=5.0.0->dash) (8.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from Werkzeug<3.1->dash) (2.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from importlib-metadata->dash) (3.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from requests->dash) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from requests->dash) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from requests->dash) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from requests->dash) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\ovaiz\\anaconda3\\lib\\site-packages (from click>=8.0->Flask<3.1,>=1.0.4->dash) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~treamlit (C:\\Users\\ovaiz\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~treamlit (C:\\Users\\ovaiz\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~treamlit (C:\\Users\\ovaiz\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install statsmodels dash dash_bootstrap_components pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2fu748_Lt8G4"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import plotly.express as px\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import dash_bootstrap_components as dbc\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v23qzQkwYwVl"
   },
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "MY3PfDGKg2MR"
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/C:/Users/ovaiz/Desktop/MACS/Semester 3/CSCI6612 - Visual Analytics/Project/Dataset-VA\u00018100205.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 210\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m processed_dfs\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# Run the function to prepare the datasets\u001b[39;00m\n\u001b[1;32m--> 210\u001b[0m processed_dfs \u001b[38;5;241m=\u001b[39m prepare_datasets(file_paths)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# Example to access processed dataframes:\u001b[39;00m\n\u001b[0;32m    213\u001b[0m cpi_data_filtered \u001b[38;5;241m=\u001b[39m processed_dfs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpi_data_filtered\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[1;32mIn[16], line 143\u001b[0m, in \u001b[0;36mprepare_datasets\u001b[1;34m(file_paths)\u001b[0m\n\u001b[0;32m    139\u001b[0m processed_dfs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, file_path \u001b[38;5;129;01min\u001b[39;00m file_paths\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;66;03m# Step 1: Read and preprocess CSV file\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m     df \u001b[38;5;241m=\u001b[39m read_and_preprocess_csv(file_path)\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;66;03m# Load and preprocess each dataset based on its specific processing needs\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpi_data\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[1;32mIn[16], line 41\u001b[0m, in \u001b[0;36mread_and_preprocess_csv\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_and_preprocess_csv\u001b[39m(file_path):\n\u001b[1;32m---> 41\u001b[0m     df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(file_path, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, multiLine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, ignoreLeadingWhiteSpace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, ignoreTrailingWhiteSpace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# Replace '--' with 0 in all columns\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/C:/Users/ovaiz/Desktop/MACS/Semester 3/CSCI6612 - Visual Analytics/Project/Dataset-VA\u00018100205.csv."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, when\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType\n",
    "import pandas as pd\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"VA Dataset Preparation\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# File paths\n",
    "file_paths = {\n",
    "    \"house_index_data\": \"Dataset-VA\\18100205.csv\",\n",
    "    \"study_permit_by_country\": \"Dataset-VA\\EN_ODP-TR-Study-IS_CITZ_sign_date(TR - SP CITZ).csv\",\n",
    "    \"temp_worker_by_province\": \"EN_ODP-TR-Work-IMP_PT_NOC4.csv\",\n",
    "    \"cpi_data\" : \"CPI_MONTHLY_BankOC.csv\"\n",
    "}\n",
    "\n",
    "# Function to preprocess CPI data, including filtering for 2015 onwards and calculating yearly averages with 2 decimal places\n",
    "def preprocess_cpi_data(file_path):\n",
    "    # Load the CPI data\n",
    "    cpi_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "    # Convert 'date' column to DateType and filter for years 2015 onwards\n",
    "    cpi_df = cpi_df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "    cpi_df = cpi_df.withColumn(\"Year\", year(col(\"date\"))).filter(col(\"Year\") >= 2015)\n",
    "\n",
    "    # Calculate yearly averages for each CPI-related column, rounding to 2 decimal places\n",
    "    cpi_avg_df = cpi_df.groupBy(\"Year\").agg(\n",
    "        round(avg(\"Total CPI\"), 2).alias(\"Avg_Total_CPI\"),\n",
    "        round(avg(\"Total_CPI_Seasonally_Adjusted)\"), 2).alias(\"Avg_Total_CPI_Seasonally_Adjusted\"),\n",
    "        round(avg(\"STATIC_TOTALCPICHANGE\"), 2).alias(\"Avg_STATIC_TOTALCPICHANGE\"),\n",
    "        round(avg(\"CPI_TRIM\"), 2).alias(\"Avg_CPI_TRIM\"),\n",
    "        round(avg(\"CPI_MEDIAN\"), 2).alias(\"Avg_CPI_MEDIAN\")\n",
    "    )\n",
    "\n",
    "    return cpi_avg_df\n",
    "\n",
    "# Function to read and preprocess CSV files\n",
    "def read_and_preprocess_csv(file_path):\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True, multiLine=True, ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True)\n",
    "\n",
    "    # Replace '--' with 0 in all columns\n",
    "    for column in df.columns:\n",
    "        column_type = df.schema[column].dataType\n",
    "\n",
    "        # Replace '--' with 0 for string or numeric columns\n",
    "        if isinstance(column_type, (StringType, IntegerType, DoubleType)):\n",
    "            df = df.withColumn(column, when(col(column) == '--', 0).otherwise(col(column)))\n",
    "\n",
    "    df = df.na.fill(0)  # Replace any remaining nulls with 0\n",
    "    return df\n",
    "\n",
    "# Reusable function to perform common transformations\n",
    "def transform_dataframe(df, rename_columns=None, skip_columns=0, convert_to_int=True):\n",
    "    # Rename columns if necessary\n",
    "    if rename_columns:\n",
    "        for old_name, new_name in rename_columns.items():\n",
    "            df = df.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "    # Convert columns to IntegerType (from `skip_columns` onward)\n",
    "    for column in df.columns[skip_columns:]:\n",
    "        column_type = df.schema[column].dataType\n",
    "\n",
    "        # Only convert columns to IntegerType if they are numeric\n",
    "        if isinstance(column_type, (IntegerType, DoubleType)):\n",
    "            df = df.withColumn(column, col(column).cast(\"int\"))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to preprocess and group study permit data by Year and Country\n",
    "def preprocess_study_permit_data(df):\n",
    "    # Rename first column to 'Country'\n",
    "    rename_columns = {df.columns[0]: \"Country\"}\n",
    "    df = transform_dataframe(df, rename_columns=rename_columns)\n",
    "\n",
    "    # Convert df2 to Pandas DataFrame for reshaping and further processing\n",
    "    df_pandas = df.toPandas()\n",
    "\n",
    "    # Reshape the dataset from wide to long format\n",
    "    df_reshaped = df_pandas.melt(id_vars=[\"Country\"], var_name=\"Date\", value_name=\"Value\")\n",
    "\n",
    "    # Convert 'Value' column to numeric, errors='coerce' will convert non-numeric values to NaN\n",
    "    df_reshaped['Value'] = pd.to_numeric(df_reshaped['Value'], errors='coerce')\n",
    "\n",
    "    # Extract Year and Month from the Date column and reformat it to YYYY-MM\n",
    "    df_reshaped['Year'] = df_reshaped['Date'].str[:2].astype(int) + 2000\n",
    "    df_reshaped['Month'] = df_reshaped['Date'].str[3:6]\n",
    "    df_reshaped['Date'] = pd.to_datetime(df_reshaped['Month'] + '-' + df_reshaped['Year'].astype(str), format='%b-%Y')\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df_reshaped.drop(columns=[\"Month\", \"Year\"], inplace=True)\n",
    "\n",
    "    # Group by Country and Year, summing up the values\n",
    "    df_grouped = df_reshaped.groupby([df_reshaped['Country'], df_reshaped['Date'].dt.year])['Value'].sum().reset_index()\n",
    "\n",
    "    df_grouped.rename(columns={'Date': 'Year'}, inplace=True)\n",
    "\n",
    "    # Convert the resulting DataFrame back to Spark DataFrame\n",
    "    df_grouped_spark = spark.createDataFrame(df_grouped)\n",
    "\n",
    "    return df_grouped_spark\n",
    "\n",
    "# Function to preprocess and group temp worker data by Province, Class Title, and Year\n",
    "def preprocess_temp_worker_data(df):\n",
    "    # Rename first two columns to 'Province' and 'Class Title'\n",
    "    rename_columns = {df.columns[0]: \"Province\", df.columns[1]: \"Class Title\"}\n",
    "    df = transform_dataframe(df, rename_columns=rename_columns, skip_columns=2)\n",
    "\n",
    "    # Convert df3 to Pandas DataFrame for reshaping and further processing\n",
    "    df_pandas = df.toPandas()\n",
    "\n",
    "    # Melt the DataFrame to long format\n",
    "    df_long_melted = pd.melt(df_pandas, id_vars=['Province', 'Class Title'],\n",
    "                             var_name='Date', value_name='Count')\n",
    "\n",
    "    # Extract Year and Month from the 'Date' column (assuming it's in the format like '15-Jan')\n",
    "    df_long_melted['Year'] = df_long_melted['Date'].str[:2].astype(int) + 2000  # Extract year (e.g., '15' -> '2015')\n",
    "    df_long_melted['Month'] = df_long_melted['Date'].str[3:]  # Extract month (e.g., 'Jan')\n",
    "\n",
    "    # Convert 'Count' to numeric, forcing any errors to NaN and then filling NaN with 0\n",
    "    df_long_melted['Count'] = pd.to_numeric(df_long_melted['Count'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    # Group by Province, Class Title, and Year, summing up the counts\n",
    "    df_grouped = df_long_melted.groupby(['Province', 'Class Title', 'Year'], as_index=False)['Count'].sum()\n",
    "\n",
    "    # Convert the resulting DataFrame back to Spark DataFrame\n",
    "    df_grouped_spark = spark.createDataFrame(df_grouped)\n",
    "\n",
    "    # Create a separate DataFrame to count occurrences of each \"Class Title\" for each year\n",
    "    class_title_count_df = df.groupBy(\"Class Title\").agg(\n",
    "        *[F.sum(F.col(year)).alias(year) for year in df.columns[2:]]\n",
    "    )\n",
    "\n",
    "    return df_grouped_spark, class_title_count_df\n",
    "\n",
    "\n",
    "def prepare_datasets(file_paths):\n",
    "    processed_dfs = {}\n",
    "\n",
    "    for key, file_path in file_paths.items():\n",
    "        # Step 1: Read and preprocess CSV file\n",
    "        df = read_and_preprocess_csv(file_path)\n",
    "\n",
    "        # Load and preprocess each dataset based on its specific processing needs\n",
    "        if key == \"cpi_data\":\n",
    "            cpi_data = preprocess_cpi_data(file_path)\n",
    "            processed_dfs[\"cpi_data_filtered\"] = cpi_data\n",
    "\n",
    "        elif key == \"house_index_data\":\n",
    "            # House index data specific transformations\n",
    "            df = transform_dataframe(df)  # Apply common transformations\n",
    "\n",
    "            # Extract only the year part of REF_DATE\n",
    "            if \"REF_DATE\" in df.columns:\n",
    "                df = df.withColumn(\"REF_DATE\", year(to_date(col(\"REF_DATE\"), \"yyyy-MM-dd\")))\n",
    "\n",
    "            # Filter by the year range (2015 to 2023)\n",
    "            start_year = 2015\n",
    "            end_year = 2023\n",
    "            df = df.filter((col(\"REF_DATE\") >= start_year) & (col(\"REF_DATE\") <= end_year))\n",
    "\n",
    "            # Drop unnecessary columns\n",
    "            columns_to_drop = ['DGUID', 'STATUS', 'SYMBOL', 'TERMINATED', 'UOM', 'UOM_ID', 'SCALAR_ID', 'VECTOR', 'DECIMALS', 'SCALAR_FACTOR', 'COORDINATE']\n",
    "            df = df.drop(*columns_to_drop)\n",
    "\n",
    "            # Group by GEO, REF_DATE, and keep other columns while calculating average of 'Value'\n",
    "            df = df.groupBy(\"REF_DATE\", \"GEO\", \"New housing price indexes\").agg(\n",
    "                F.avg(\"Value\").alias(\"Avg_Value\"),\n",
    "                *[F.first(c).alias(c) for c in df.columns if c not in [\"REF_DATE\", \"GEO\", \"VALUE\", \"New housing price indexes\"]]  # Retain all other columns\n",
    "            )\n",
    "\n",
    "            # Round the avg_house_index to 2 decimal places\n",
    "            df = df.withColumn(\"Avg_Value\", F.round(col(\"Avg_Value\"), 2))\n",
    "\n",
    "            # Rename columns for readability\n",
    "            df = df.withColumnRenamed(\"GEO\", \"Location\") \\\n",
    "            .withColumnRenamed(\"REF_DATE\", \"Year\") \\\n",
    "            .withColumnRenamed(\"New housing price indexes\", \"New Housing Price Index\")\n",
    "\n",
    "            # Handle cases where Location has no comma or extra spaces\n",
    "            df = df.withColumn(\n",
    "                \"Province\",\n",
    "                F.when(\n",
    "                    F.col(\"Location\").contains(\",\"),  # Check if Location contains a comma\n",
    "                   F.when(F.col(\"Location\").contains(\",\"), F.split(F.trim(df[\"Location\"]), \",\")[1])  # Extract the part after the comma\n",
    "                ).otherwise(F.col(\"Location\"))  # If no comma, keep the Location value itself\n",
    "            )\n",
    "\n",
    "            # Optionally clean up by trimming spaces in Province\n",
    "            df = df.withColumn(\"Province\", F.trim(df[\"Province\"]))\n",
    "\n",
    "        elif key == \"study_permit_by_country\":\n",
    "            # Process study permit data using the preprocessing function\n",
    "            study_grouped = preprocess_study_permit_data(df)\n",
    "            processed_dfs[\"study_permit_grouped_by_year\"] = study_grouped\n",
    "\n",
    "        elif key == \"temp_worker_by_province\":\n",
    "            # Process temp worker data and obtain both grouped and class title count DataFrames\n",
    "            temp_worker_grouped, class_title_count_df = preprocess_temp_worker_data(df)\n",
    "            processed_dfs[\"temp_worker_grouped_by_year\"] = temp_worker_grouped\n",
    "            processed_dfs[\"class_title_count_per_year\"] = class_title_count_df\n",
    "\n",
    "        # Store processed dataframe in dictionary\n",
    "        processed_dfs[key] = df\n",
    "\n",
    "    return processed_dfs\n",
    "\n",
    "# Run the function to prepare the datasets\n",
    "processed_dfs = prepare_datasets(file_paths)\n",
    "\n",
    "# Example to access processed dataframes:\n",
    "cpi_data_filtered = processed_dfs[\"cpi_data_filtered\"]\n",
    "house_index_data = processed_dfs[\"house_index_data\"]\n",
    "study_permit_data = processed_dfs[\"study_permit_by_country\"]\n",
    "temp_worker_data = processed_dfs[\"temp_worker_by_province\"]\n",
    "study_permit_grouped_by_year = processed_dfs[\"study_permit_grouped_by_year\"]\n",
    "temp_worker_grouped_by_year = processed_dfs[\"temp_worker_grouped_by_year\"]\n",
    "class_title_count_per_year = processed_dfs[\"class_title_count_per_year\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FoDos_XRj8rk",
    "outputId": "b4770bff-b27c-4286-d29b-fb8848c7e395"
   },
   "outputs": [],
   "source": [
    "house_index_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T8G52MMOj-_S",
    "outputId": "3043edac-8fc4-4103-db0d-7f8d8d94cdce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+\n",
      "|        _c0|15-Jan|15-Feb|15-Mar|15-Apr|15-May|15-Jun|15-Jul|15-Aug|15-Sep|15-Oct|15-Nov|15-Dec|16-Jan|16-Feb|16-Mar|16-Apr|16-May|16-Jun|16-Jul|16-Aug|16-Sep|16-Oct|16-Nov|16-Dec|17-Jan|17-Feb|17-Mar|17-Apr|17-May|17-Jun|17-Jul|17-Aug|17-Sep|17-Oct|17-Nov|17-Dec|18-Jan|18-Feb|18-Mar|18-Apr|18-May|18-Jun|18-Jul|18-Aug|18-Sep|18-Oct|18-Nov|18-Dec|19-Jan|19-Feb|19-Mar|19-Apr|19-May|19-Jun|19-Jul|19-Aug|19-Sep|19-Oct|19-Nov|19-Dec|20-Jan|20-Feb|20-Mar|20-Apr|20-May|20-Jun|20-Jul|20-Aug|20-Sep|20-Oct|20-Nov|20-Dec|21-Jan|21-Feb|21-Mar|21-Apr|21-May|21-Jun|21-Jul|21-Aug|21-Sep|21-Oct|21-Nov|21-Dec|22-Jan|22-Feb|22-Mar|22-Apr|22-May|22-Jun|22-Jul|22-Aug|22-Sep|22-Oct|22-Nov|22-Dec|23-Jan|23-Feb|23-Mar|23-Apr|23-May|23-Jun|23-Jul|23-Aug|23-Sep|23-Oct|23-Nov|23-Dec|24-Jan|24-Feb|24-Mar|24-Apr|24-May|24-Jun|24-Jul|\n",
      "+-----------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+\n",
      "|Afghanistan|    10|     0|     0|    10|     5|     5|    10|    10|    15|     5|    10|    10|    10|     0|    10|     0|    10|    15|    10|    10|    15|     5|     5|    15|    15|     0|     5|     5|     5|     5|    10|    10|    20|     5|     5|     5|    10|     0|     0|    10|     0|     5|    10|    10|    10|    10|     5|     0|    10|     0|    10|     5|    10|     0|    10|    15|    15|     5|     0|     5|     0|     0|     5|     5|    15|     0|    10|    10|    15|     5|     5|     5|    10|     0|     0|     5|     5|    15|    10|     5|    10|     0|     5|     5|     5|    10|    15|     5|    10|    10|    15|    40|    35|    10|    20|     0|    15|    10|    15|    10|     5|    10|    10|    25|    30|    10|     5|     5|    15|    15|     0|    10|     5|    10|     5|\n",
      "|    Albania|    10|     5|     0|    10|     5|     5|    10|    15|    20|    10|     0|    15|    15|     0|    10|     5|    10|    20|    15|    25|    20|    10|    20|    15|    10|     0|    10|    10|    10|    15|    15|    50|    25|    15|    15|    15|    15|    10|     0|    15|    20|    10|    30|    55|    25|    20|    20|    25|    25|    25|    15|    20|    10|    25|    45|    85|    35|    30|    20|    40|    35|     5|    20|    15|    45|    25|    25|    20|    20|    15|    15|    20|    25|    15|    35|    20|    40|    25|    25|    30|    40|    15|    20|    25|    30|    15|    20|    20|    15|    20|    30|    65|    50|    20|    20|    40|    55|    30|    25|    35|    35|    40|    55|   100|    70|    45|    50|    45|    55|    30|    30|    30|    25|    40|    45|\n",
      "|    Algeria|    60|    40|    55|    55|    55|    45|   120|   170|   135|   100|   105|   135|    75|    60|    35|    45|    60|    50|    40|   140|   135|    60|    45|   100|   100|    35|    40|    35|    50|    35|    90|   185|   170|    70|    80|   135|   125|    50|    50|   100|    70|    85|   140|   270|   215|   130|    95|   160|   270|    75|    80|    90|    70|   125|   160|   485|   540|   210|   190|   390|   365|    80|    55|   105|   175|   105|    95|   175|   245|    70|   110|   600|   230|    75|   145|   115|   155|   100|   360|   790|   195|    80|   105|   830|   210|   145|   115|   280|   165|   170|   385| 1,495|   640|   190|   360| 1,255|   560|   180|   250|   465|   175|   400| 1,485| 2,045|   595|   255|   785| 2,405| 1,000|   465|   580|   700|   330|   560| 1,665|\n",
      "|    Andorra|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     5|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     5|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0|\n",
      "|     Angola|    15|     0|     0|     0|     0|     0|     0|    10|     0|    10|    10|    10|    25|     0|     0|     0|     5|    20|     0|    10|     5|     0|     0|     0|     0|     0|     0|     0|     0|     5|     0|     0|    10|     0|     5|     0|     5|     0|     0|     0|     0|     0|     0|     5|     0|     0|    10|     0|     0|    10|     5|     5|    30|    10|    10|    40|     5|     0|     0|     0|    10|     0|     0|     0|     0|     0|     0|    10|    10|     0|     0|     0|     0|     0|     0|     0|     0|     0|     5|     5|     5|     5|     5|    10|     0|     0|    10|     0|     5|     5|    10|    10|    10|    15|     5|     0|     5|     0|    10|     5|     5|     5|    10|    10|     0|     0|     5|     5|     5|     5|    10|     5|     0|     5|    10|\n",
      "+-----------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "study_permit_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W1tabw5OrKbE",
    "outputId": "64161e67-e50c-4318-c6be-a2a43e63a082"
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1060.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 15) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m study_permit_grouped_by_year\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_string(n, truncate, vertical))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    962\u001b[0m     )\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;241m20\u001b[39m, vertical)\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1060.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 15) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n"
     ]
    }
   ],
   "source": [
    "study_permit_grouped_by_year.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "czr9u5wSkBqz",
    "outputId": "f0d0a0e0-5324-4ece-eb44-fdcaa0a6b39f"
   },
   "outputs": [],
   "source": [
    "temp_worker_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-8n47CpSEIUA",
    "outputId": "aa0406a4-5506-4d58-cbf4-754ce21b9cbb"
   },
   "outputs": [],
   "source": [
    "temp_worker_grouped_by_year.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3Pd3ZAZLCH4",
    "outputId": "63c22399-b518-495c-a354-cb4d3c57101f"
   },
   "outputs": [],
   "source": [
    "cpi_data_filtered.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfiSH_-ux9jr",
    "outputId": "6f7f076d-de5d-4bf3-bb1f-161b361f082b"
   },
   "outputs": [],
   "source": [
    "# Aggregate by Country and Date (Year), summing the 'Value' column for study permits\n",
    "df_aggregated = study_permit_grouped_by_year.groupBy(\"Year\").agg(\n",
    "    F.sum(\"Value\").alias(\"Number_of_Study_Permits_Issued\")\n",
    ")\n",
    "\n",
    "# Join the aggregated study permit data with house_index_data on the 'Year' column\n",
    "df_combined = house_index_data.join(df_aggregated, on=\"Year\", how=\"left\")\n",
    "\n",
    "# Calculate the Impact Metric: Ratio of Study Permits Issued to Housing Price Index\n",
    "df_combined = df_combined.withColumn(\n",
    "    \"Study_Permit_To_Value_Impact_Metric\",\n",
    "    F.col(\"Number_of_Study_Permits_Issued\") / F.col(\"Avg_Value\")\n",
    ")\n",
    "\n",
    "# Round the Study_Permit_To_Value_Impact_Metric to 2 decimal places\n",
    "df_combined = df_combined.withColumn(\"Study_Permit_To_Value_Impact_Metric\", F.round(F.col(\"Study_Permit_To_Value_Impact_Metric\"), 2))\n",
    "\n",
    "# Define the mapping with individual `when` clauses\n",
    "df_combined = df_combined.withColumn(\n",
    "    \"Province\",\n",
    "    when(F.col(\"Province\") == \"Quebec part\", \"Quebec\")\n",
    "    .when(F.col(\"Province\") == \"Ontario part\", \"Ontario\")\n",
    "    .when(F.col(\"Province\") == \"Fredericton\", \"New Brunswick\")\n",
    "    .when(F.col(\"Province\") == \"Prairie Region\", \"Alberta\")\n",
    "    .when(F.col(\"Province\") == \"Atlantic Region\", \"Nova Scotia\")  # Adjust if necessary\n",
    "    .when(F.col(\"Province\") == \"Canada\", None)  # Exclude \"Canada\" from provinces\n",
    "    .otherwise(F.col(\"Province\"))\n",
    ")\n",
    "\n",
    "# Aggregate total count of temporary workers by Year and Province\n",
    "workers_aggregated = temp_worker_grouped_by_year.groupBy(\"Year\", \"Province\").agg(\n",
    "    F.sum(\"Count\").alias(\"Total_Workers_Count\")\n",
    ")\n",
    "\n",
    "# Count the number of unique Class Titles by Year and Province\n",
    "class_titles_count = temp_worker_grouped_by_year.groupBy(\"Year\", \"Province\").agg(\n",
    "    F.countDistinct(\"Class Title\").alias(\"Number_of_Class_Titles\")\n",
    ")\n",
    "\n",
    "# Find the most common Class Title per Year and Province\n",
    "most_common_class = temp_worker_grouped_by_year.groupBy(\"Year\", \"Province\").agg(\n",
    "    F.first(\"Class Title\").alias(\"Most_Common_Class_Title\")\n",
    ")\n",
    "\n",
    "# Join the workers, class titles count, and common class data with the existing DataFrame\n",
    "df_combined = df_combined \\\n",
    "    .join(workers_aggregated, on=[\"Year\", \"Province\"], how=\"left\") \\\n",
    "    .join(class_titles_count, on=[\"Year\", \"Province\"], how=\"left\") \\\n",
    "    .join(most_common_class, on=[\"Year\", \"Province\"], how=\"left\")\n",
    "\n",
    "# Calculate the Impact Metric: Ratio of Workers Count to Housing Price Index\n",
    "df_combined = df_combined.withColumn(\n",
    "    \"Workers_To_Housing_Impact_Ratio\",\n",
    "    F.col(\"Total_Workers_Count\") / F.col(\"Avg_Value\")\n",
    ")\n",
    "\n",
    "# Round the Workers_To_Housing_Impact_Ratio to 2 decimal places\n",
    "df_combined = df_combined.withColumn(\"Workers_To_Housing_Impact_Ratio\", F.round(F.col(\"Workers_To_Housing_Impact_Ratio\"), 2))\n",
    "\n",
    "# Add combined impact ratio (students and workers)\n",
    "df_combined = df_combined.withColumn(\n",
    "    \"Combined_Impact_Ratio\",\n",
    "    (F.col(\"Study_Permit_To_Value_Impact_Metric\") + F.col(\"Workers_To_Housing_Impact_Ratio\")) / 2\n",
    ")\n",
    "\n",
    "# Round the Combined_Impact_Ratio to 2 decimal places\n",
    "df_combined = df_combined.withColumn(\"Combined_Impact_Ratio\", F.round(F.col(\"Combined_Impact_Ratio\"), 2))\n",
    "\n",
    "# Join with CPI data\n",
    "df_combined = df_combined.join(cpi_data_filtered, on=\"Year\", how=\"left\")\n",
    "\n",
    "# Display the combined DataFrame with all the added columns\n",
    "df_combined.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fiUqcOq-SwrQ"
   },
   "outputs": [],
   "source": [
    "# import dash\n",
    "# from dash import dcc, html, Input, Output\n",
    "# import pandas as pd\n",
    "# import plotly.express as px\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming df_combined is a DataFrame, converting it to pandas DataFrame\n",
    "# data = df_combined.toPandas()\n",
    "\n",
    "# # Remove rows where 'Province' is null or NaN\n",
    "# data = data.dropna(subset=[\"Province\"])\n",
    "\n",
    "# # Filter out only the numeric columns for correlation calculation\n",
    "# numeric_data = data.select_dtypes(include=[np.number])\n",
    "# correlation_matrix = numeric_data.corr()\n",
    "\n",
    "# # Initialize the Dash app\n",
    "# app = dash.Dash(__name__)\n",
    "\n",
    "# app.layout = html.Div([\n",
    "#     html.H1(\"Correlation Analysis for Housing Prices and Temporary Residents\"),\n",
    "\n",
    "#     # Dropdown to select province\n",
    "#     html.Div([\n",
    "#         html.Label(\"Select Province:\"),\n",
    "#         dcc.Dropdown(\n",
    "#             id=\"province-dropdown\",\n",
    "#             options=[{\"label\": province, \"value\": province} for province in data[\"Province\"].unique()],\n",
    "#             value=\"British Columbia\",\n",
    "#             multi=False,\n",
    "#             clearable=False,\n",
    "#             style={'width': '50%'}\n",
    "#         ),\n",
    "#     ], style={'padding': '10px'}),\n",
    "\n",
    "#     # Correlation heatmap\n",
    "#     html.Div([\n",
    "#         dcc.Graph(id=\"correlation-heatmap\")\n",
    "#     ], style={'padding': '20px', 'width': '90%', 'margin': 'auto'}),\n",
    "# ])\n",
    "\n",
    "# # Callback for the correlation heatmap\n",
    "# @app.callback(\n",
    "#     Output(\"correlation-heatmap\", \"figure\"),\n",
    "#     Input(\"province-dropdown\", \"value\")\n",
    "# )\n",
    "# def update_correlation_heatmap(selected_province):\n",
    "#     # Filter the data based on the selected province\n",
    "#     province_data = data[data[\"Province\"] == selected_province]\n",
    "#     # Only numeric columns for correlation calculation\n",
    "#     numeric_province_data = province_data.select_dtypes(include=[np.number])\n",
    "#     corr_matrix = numeric_province_data.corr()\n",
    "\n",
    "#     # Create the heatmap using Plotly\n",
    "#     fig = px.imshow(\n",
    "#         corr_matrix,\n",
    "#         text_auto=True,\n",
    "#         color_continuous_scale=\"Viridis\",\n",
    "#         labels={\"color\": \"Correlation\"},\n",
    "#         title=f\"Correlation Heatmap for {selected_province}\",\n",
    "#         width=800,  # Width of the heatmap in pixels\n",
    "#         height=600  # Height of the heatmap in pixels\n",
    "#     )\n",
    "#     return fig\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K0G3s97PaBXA",
    "outputId": "a0273abf-2505-469b-b1e7-5906f9b4c9ee"
   },
   "outputs": [],
   "source": [
    "df_combined.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "nwYgutldZ0Dv",
    "outputId": "bda4f9b7-0cfe-41ea-cd0b-b14ddab2a1e2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dash\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataframe (df_combined)\n",
    "# df_combined = df_combined.toPandas()\n",
    "\n",
    "# Convert 'Year' to datetime\n",
    "df_combined['Year'] = pd.to_datetime(df_combined['Year'], format='%Y')\n",
    "\n",
    "# Initialize the Dash app\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "\n",
    "# Define the layout of the dashboard\n",
    "app.layout = dbc.Container(\n",
    "    [\n",
    "        html.H1(\"Housing Price Analysis Dashboard\", style={'text-align': 'center'}),\n",
    "        html.Hr(),\n",
    "\n",
    "        # 3x2 Grid Layout\n",
    "        dbc.Row(\n",
    "            [\n",
    "                dbc.Col([  # Factors Affecting Housing Prices\n",
    "                    html.H4(\"Factors Affecting Housing Prices\"),\n",
    "                    dcc.Dropdown(\n",
    "                        id='province-dropdown',\n",
    "                        options=[{'label': i, 'value': i} for i in df_combined['Province'].unique()],\n",
    "                        multi=True,\n",
    "                        value=['Ontario']\n",
    "                    ),\n",
    "                    dcc.Graph(id='scatter-factors')\n",
    "                ], md=6),\n",
    "\n",
    "                dbc.Col([  # Forecasting Housing Prices (ARIMA)\n",
    "                    html.H4(\"Forecasting Housing Prices\"),\n",
    "                    dcc.Graph(id='forecast-chart'),\n",
    "                    dcc.Slider(\n",
    "                        id='forecast-slider',\n",
    "                        min=1,\n",
    "                        max=5,\n",
    "                        step=1,\n",
    "                        marks={i: str(i) for i in range(1, 6)},\n",
    "                        value=1\n",
    "                    )\n",
    "                ], md=6),\n",
    "            ],\n",
    "            className=\"mb-4\"\n",
    "        ),\n",
    "\n",
    "        # 2x2 Grid Layout for clustering and anomaly detection\n",
    "        dbc.Row(\n",
    "            [\n",
    "                dbc.Col([  # Regional Clustering\n",
    "                    html.H4(\"Regional Clustering\"),\n",
    "                    dcc.Graph(id='cluster-map'),\n",
    "                ], md=6),\n",
    "\n",
    "                dbc.Col([  # Anomaly Detection\n",
    "                    html.H4(\"Anomaly Detection\"),\n",
    "                    dcc.Graph(id='anomaly-chart'),\n",
    "                ], md=6),\n",
    "            ],\n",
    "            className=\"mb-4\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Callback for visualizing factors affecting housing prices\n",
    "@app.callback(\n",
    "    Output('scatter-factors', 'figure'),\n",
    "    [Input('province-dropdown', 'value')]\n",
    ")\n",
    "def update_scatter(selected_provinces):\n",
    "    df_filtered = df_combined[df_combined['Province'].isin(selected_provinces)]\n",
    "    fig = px.scatter(df_filtered, x='Study_Permit_To_Value_Impact_Metric',\n",
    "                     y='New Housing Price Index', color='Province',\n",
    "                     title=\"Study Permits vs Housing Price Index\")\n",
    "    return fig\n",
    "\n",
    "# Callback for forecasting housing prices (ARIMA)\n",
    "@app.callback(\n",
    "    Output('forecast-chart', 'figure'),\n",
    "    [Input('forecast-slider', 'value')]\n",
    ")\n",
    "def update_forecast(years_to_forecast):\n",
    "    df_sorted = df_combined.sort_values('Year')\n",
    "    model = ARIMA(df_sorted['New Housing Price Index'], order=(1, 1, 1))\n",
    "    model_fit = model.fit()\n",
    "    forecast = model_fit.forecast(steps=years_to_forecast)\n",
    "    forecast_years = pd.date_range(start=df_sorted['Year'].max(), periods=years_to_forecast+1, freq='Y')[1:]\n",
    "\n",
    "    fig = px.line(x=forecast_years, y=forecast, title=\"Forecasted Housing Prices\")\n",
    "    return fig\n",
    "\n",
    "# Callback for clustering regions\n",
    "@app.callback(\n",
    "    Output('cluster-map', 'figure'),\n",
    "    [Input('province-dropdown', 'value')]\n",
    ")\n",
    "def update_clustering(selected_provinces):\n",
    "    df_filtered = df_combined[df_combined['Province'].isin(selected_provinces)]\n",
    "    kmeans = KMeans(n_clusters=3)\n",
    "    clusters = kmeans.fit_predict(df_filtered[['New Housing Price Index', 'Avg_Value', 'Total_Workers_Count']])\n",
    "\n",
    "    df_filtered['Cluster'] = clusters\n",
    "    fig = px.scatter(df_filtered, x='New Housing Price Index',\n",
    "                     y='Avg_Value', color='Cluster',\n",
    "                     title=\"Housing Price Clusters\")\n",
    "    return fig\n",
    "\n",
    "# Callback for anomaly detection\n",
    "@app.callback(\n",
    "    Output('anomaly-chart', 'figure'),\n",
    "    [Input('province-dropdown', 'value')]\n",
    ")\n",
    "def update_anomalies(selected_provinces):\n",
    "    df_filtered = df_combined[df_combined['Province'].isin(selected_provinces)]\n",
    "    model = IsolationForest(contamination=0.1)\n",
    "    df_filtered['Anomaly'] = model.fit_predict(df_filtered[['New Housing Price Index']])\n",
    "\n",
    "    fig = px.scatter(df_filtered, x='Year',\n",
    "                     y='New Housing Price Index', color='Anomaly',\n",
    "                     title=\"Anomaly Detection in Housing Prices\")\n",
    "    return fig\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bvx9WicWbnPG"
   },
   "outputs": [],
   "source": [
    "# import dash\n",
    "# from dash import dcc, html, Input, Output\n",
    "# import pandas as pd\n",
    "# import plotly.express as px\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# # Sample DataFrame - Replace this with your actual combined DataFrame\n",
    "# # Assuming df_combined is your combined data with numeric columns for clustering\n",
    "# df_combined = pd.DataFrame({\n",
    "#     'Feature1': [1, 2, 3, 4, 5, 6],\n",
    "#     'Feature2': [2, 3, 4, 5, 6, 7],\n",
    "#     'Feature3': [5, 4, 3, 2, 1, 0],\n",
    "# })\n",
    "\n",
    "# # Initialize the Dash app\n",
    "# app = dash.Dash(__name__)\n",
    "\n",
    "# app.layout = html.Div([\n",
    "#     html.H1(\"Clustering Analysis Dashboard\"),\n",
    "\n",
    "#     # Dropdown to select number of clusters\n",
    "#     html.Div([\n",
    "#         html.Label(\"Select Number of Clusters:\"),\n",
    "#         dcc.Slider(\n",
    "#             id=\"num-clusters\",\n",
    "#             min=2,\n",
    "#             max=10,\n",
    "#             step=1,\n",
    "#             value=3,\n",
    "#             marks={i: str(i) for i in range(2, 11)},\n",
    "#         ),\n",
    "#     ], style={'padding': '10px'}),\n",
    "\n",
    "#     # Dropdowns to select features for clustering\n",
    "#     html.Div([\n",
    "#         html.Label(\"Select Feature for X-axis:\"),\n",
    "#         dcc.Dropdown(\n",
    "#             id=\"x-axis-feature\",\n",
    "#             options=[{\"label\": col, \"value\": col} for col in df_combined.columns],\n",
    "#             value=\"Feature1\",  # Default to the first feature\n",
    "#             multi=False,\n",
    "#             clearable=False,\n",
    "#             style={'width': '45%', 'display': 'inline-block'}\n",
    "#         ),\n",
    "#         html.Label(\"Select Feature for Y-axis:\"),\n",
    "#         dcc.Dropdown(\n",
    "#             id=\"y-axis-feature\",\n",
    "#             options=[{\"label\": col, \"value\": col} for col in df_combined.columns],\n",
    "#             value=\"Feature2\",  # Default to the second feature\n",
    "#             multi=False,\n",
    "#             clearable=False,\n",
    "#             style={'width': '45%', 'display': 'inline-block', 'marginLeft': '10px'}\n",
    "#         ),\n",
    "#     ], style={'padding': '10px'}),\n",
    "\n",
    "#     # Scatter plot for clustering visualization\n",
    "#     html.Div([\n",
    "#         dcc.Graph(id=\"cluster-plot\")\n",
    "#     ], style={'padding': '20px'}),\n",
    "# ])\n",
    "\n",
    "# # Callback to update the clustering plot\n",
    "# @app.callback(\n",
    "#     Output(\"cluster-plot\", \"figure\"),\n",
    "#     [Input(\"num-clusters\", \"value\"),\n",
    "#      Input(\"x-axis-feature\", \"value\"),\n",
    "#      Input(\"y-axis-feature\", \"value\")]\n",
    "# )\n",
    "# def update_cluster_plot(num_clusters, x_feature, y_feature):\n",
    "#     # Select the features for clustering\n",
    "#     X = df_combined[[x_feature, y_feature]]\n",
    "\n",
    "#     # Ensure the features are numeric and scale them\n",
    "#     scaler = StandardScaler()\n",
    "#     X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "#     # Perform clustering\n",
    "#     kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "#     clusters = kmeans.fit_predict(X_scaled)\n",
    "#     df_combined[\"Cluster\"] = clusters\n",
    "\n",
    "#     # Create the scatter plot\n",
    "#     fig = px.scatter(\n",
    "#         df_combined,\n",
    "#         x=x_feature,\n",
    "#         y=y_feature,\n",
    "#         color=\"Cluster\",\n",
    "#         title=f\"Clustering Analysis with {num_clusters} Clusters\",\n",
    "#         color_continuous_scale=\"Viridis\",\n",
    "#         labels={\"Cluster\": \"Cluster\"},\n",
    "#     )\n",
    "\n",
    "#     # Add cluster centers to the plot\n",
    "#     cluster_centers = kmeans.cluster_centers_\n",
    "#     fig.add_scatter(\n",
    "#         x=cluster_centers[:, 0], y=cluster_centers[:, 1],\n",
    "#         mode=\"markers\",\n",
    "#         marker=dict(color=\"red\", size=10, symbol=\"x\"),\n",
    "#         name=\"Cluster Centers\"\n",
    "#     )\n",
    "\n",
    "#     # Customize layout\n",
    "#     fig.update_layout(\n",
    "#         xaxis_title=x_feature,\n",
    "#         yaxis_title=y_feature,\n",
    "#         template=\"plotly_white\"\n",
    "#     )\n",
    "\n",
    "#     return fig\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDJ7vSPe8dXl"
   },
   "outputs": [],
   "source": [
    "# # Get all unique values in the 'Province' column\n",
    "# unique_provinces = df_combined.select(\"Province\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "# print(unique_provinces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNXIn3rFx8Hj"
   },
   "outputs": [],
   "source": [
    "# # Check for null values in each column of the DataFrame\n",
    "# df_combined.select([F.count(F.when(col(c).isNull(), c)).alias(c) for c in df_combined.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srV_y1uxu3kC"
   },
   "source": [
    "#### Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-g1gISHdukhQ"
   },
   "outputs": [],
   "source": [
    "# # Convert to Pandas DataFrame for easier manipulation\n",
    "# pdf = house_index_data.toPandas()\n",
    "\n",
    "# # Group by REF_DATE and GEO, taking the average value\n",
    "# time_series_data = pdf.groupby(['REF_DATE', 'GEO']).agg({'VALUE': 'mean'}).reset_index()\n",
    "\n",
    "# fig = px.line(time_series_data, x='REF_DATE', y='VALUE', color='GEO',\n",
    "#               title='Average Housing Price Index Over Time',\n",
    "#               labels={'VALUE': 'Average Housing Price Index', 'REF_DATE': 'Date'})\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6l8rubdAu6Ex"
   },
   "source": [
    "#### Compare Housing Price Indexes Across Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rzt4FbwvYsA"
   },
   "outputs": [],
   "source": [
    "# # Aggregating data by GEO\n",
    "# region_comparison = pdf.groupby('GEO').agg({'VALUE': 'mean'}).reset_index()\n",
    "\n",
    "# # Plotting comparison of housing prices by region\n",
    "# fig2 = px.bar(region_comparison, x='GEO', y='VALUE',\n",
    "#                title='Average Housing Price Index by Region',\n",
    "#                labels={'VALUE': 'Average Housing Price Index', 'GEO': 'Region'})\n",
    "# fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6DUn807wvr8z"
   },
   "outputs": [],
   "source": [
    "# # Set REF_DATE as index\n",
    "# # time_series_data = time_series_data.asfreq('MS')\n",
    "# time_series_data.set_index('REF_DATE', inplace=True)\n",
    "# ts = time_series_data[time_series_data['GEO'] == 'Canada']['VALUE']\n",
    "\n",
    "# # Fit ARIMA model (parameters (p, d, q) should be tuned)\n",
    "# model = ARIMA(ts, order=(1, 1, 1))\n",
    "# model_fit = model.fit()\n",
    "\n",
    "# # Make predictions\n",
    "# forecast = model_fit.forecast(steps=12)  # Forecasting next 12 months\n",
    "\n",
    "# # Plot the results\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(ts, label='Historical Data')\n",
    "# plt.plot(forecast, label='Forecast', color='red')\n",
    "# plt.title('ARIMA Forecast for Canada Housing Price Index')\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Housing Price Index')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-j76j8iwRxx"
   },
   "outputs": [],
   "source": [
    "# # Initialize Spark session\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Housing Price Clustering\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# # Assuming df1 is your input DataFrame\n",
    "# # Group by GEO and calculate the average VALUE\n",
    "# region_data = house_index_data.groupBy(\"GEO\").agg(avg(\"VALUE\").alias(\"avg_value\"))\n",
    "\n",
    "# # Convert to Pandas DataFrame for clustering\n",
    "# region_pdf = region_data.toPandas()\n",
    "\n",
    "# # Create a feature vector from avg_value\n",
    "# assembler = VectorAssembler(inputCols=[\"avg_value\"], outputCol=\"features\")\n",
    "# region_data_vector = assembler.transform(region_data)\n",
    "\n",
    "# # Apply KMeans clustering\n",
    "# kmeans = KMeans(k=3, seed=1)  # 3 clusters\n",
    "# model = kmeans.fit(region_data_vector)\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = model.transform(region_data_vector)\n",
    "\n",
    "# # Convert predictions to Pandas DataFrame\n",
    "# predictions_pdf = predictions.select(\"GEO\", \"avg_value\", \"prediction\").toPandas()\n",
    "\n",
    "# # Define cluster labels\n",
    "# label_map = {0: 'High', 1: 'Low', 2: 'Medium'}\n",
    "# predictions_pdf['cluster_label'] = predictions_pdf['prediction'].map(label_map)\n",
    "\n",
    "# # Plotting the clusters based on region\n",
    "# fig = px.scatter(predictions_pdf, x='GEO', y='avg_value', color='cluster_label',\n",
    "#                  title='K-Means Clustering of Housing Price Index by Region',\n",
    "#                  labels={'avg_value': 'Average Housing Price Index', 'GEO': 'Region'},\n",
    "#                  color_discrete_sequence=px.colors.qualitative.Set1)\n",
    "\n",
    "# # Customize layout for better visibility\n",
    "# fig.update_traces(marker=dict(size=12))\n",
    "# fig.update_layout(xaxis_title='Region', yaxis_title='Average Housing Price Index')\n",
    "\n",
    "# # Show plot\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWujQP4FfX8h"
   },
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
